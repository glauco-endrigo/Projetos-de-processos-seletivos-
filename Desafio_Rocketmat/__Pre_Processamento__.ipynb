{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento <br />\n",
    "\n",
    "#### Bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classe 1: Pre_Processamento_inicial:\n",
    "Essa classe é usada para limpar os dados e retirar colunas que se mostram não relevantes para o objetivo inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pre_Processamento_inicial:\n",
    "    def __init__(self, data,  Premissas_1 = True):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.Premissas_1 = Premissas_1\n",
    "        self.data        = data\n",
    "        self.colunas_desconsideradas_premissas_1 =  [line.rstrip('\\n') for line in open(\"colunas_desconsideradas_premissas_1.txt\")]\n",
    " \n",
    "       \n",
    "        if self.Premissas_1:\n",
    "            self.dados_processados_inicialmente = ((self.data).drop(columns = self.colunas_desconsideradas_premissas_1).fillna(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classe 2: Separar\n",
    "* Separa os dados em X, y para o modelo ML  \n",
    "* Separa as colunas para aplicar Enconding e Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Separar:\n",
    "    def separando(self, TARGET, df ): \n",
    "        \n",
    "        # Separando X e y para o modelo ML\n",
    "        self.TARGET = TARGET\n",
    "        self.X = df \n",
    "        self.y = df[self.TARGET]\n",
    "        self.X = self.X.drop(columns=[self.TARGET])\n",
    "\n",
    "        # Separando as colunas para aplicar Enconding e Scaling\n",
    "        X_dtypes = self.X.dtypes\n",
    "        \n",
    "        # Para o Enconding\n",
    "        self.Colunas_dados_categoricos = X_dtypes.loc[(X_dtypes == object)]\n",
    "        self.Colunas_dados_categoricos = list(self.Colunas_dados_categoricos.index)\n",
    "        \n",
    "        # Para o Scaling\n",
    "        self.Colunas_dados_numericos = X_dtypes.loc[(X_dtypes != object)]\n",
    "        self.Colunas_dados_numericos = list(self.Colunas_dados_numericos.index)\n",
    "        \n",
    "\n",
    "        \n",
    "# Caso no futuro eu queira fazer o Encoding manualmente:\n",
    "\n",
    "# column_trans = make_column_transformer((OneHotEncoder(), self.Colunas_converter_dados_categoricos), remainder = \"passthrough\")\n",
    "# self.X = column_trans.fit_transform(self.X)\n",
    "# self.X = pd.DataFrame(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classe 3: Remover_colunas_redundantes\n",
    "Caso se queira tirar a redundância que o One Hot Encoder() gera. Testei essa função no projeto e foi muito promissora, mas ainda preciso integrar nas pipelines, lá o encoder é aplicado de uma maneira um pouco diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Remover_colunas_redundantes:\n",
    "    \n",
    "    def removendo(self, dados, X ): #dado_treinar\n",
    "   \n",
    "        X_dtypes = dados.dtypes\n",
    "        Colunas_dados_categoricos = X_dtypes.loc[(X_dtypes == object)]\n",
    "        Colunas_dados_categoricos = list(Colunas_dados_categoricos.index)\n",
    "\n",
    "        conjunto = {} # Dicionário {nome da coluna:categorias da coluna} \n",
    "        col = []      # lista com qtd de categorias de cada coluna\n",
    "        k=[]          # lista para ser usada na redução de colunas dos dados \n",
    "\n",
    "        def soma(x,y): \n",
    "            return x+y\n",
    "\n",
    "        for i in Colunas_dados_categoricos:\n",
    "\n",
    "            conjunto[i] = (set(dados[i]))\n",
    "            variable    = len(conjunto[i])\n",
    "\n",
    "            if i == Colunas_dados_categoricos[0]:\n",
    "                col.append(variable-1) \n",
    "                k.append(reduce(soma,col))\n",
    "            else:\n",
    "                col.append(variable) \n",
    "                k.append(reduce(soma,col))\n",
    "        self.resultado = k\n",
    "        \n",
    "        self.X = X.drop(columns = k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
